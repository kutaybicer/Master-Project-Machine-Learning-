import numpy as np 
import pandas as pd 
import os
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Dropout
from tensorflow.keras.utils import plot_model
from tensorflow.keras.optimizers import Adam 
import sys
import sklearn
from sklearn.preprocessing import MinMaxScaler
import matplotlib 
import matplotlib.pyplot as plt 
import tensorflow as tf
import cv2
from google.cloud import storage
from PIL import Image

tf.test.gpu_device_name()
tf.test.is_gpu_available()

#Defining environment for cloud

os.environ['GOOGLE_APPlICATION_CREDENTIALS'] = 'C:\Project\project-358014-1ab403deb150.json'

#Creating storage client

storage_client = storage.Client()

#Creating bucket in the cloud for storage

# bucket_name = 'ganimages'
# bucket = storage_client.bucket(bucket_name)
# bucket.create(location='EU')

#Upload function for cloud

def upload_cloud(blob_name, path, bucket_name):
    try:
        bucket =storage_client.get_bucket(bucket_name)
        blob = bucket.blob(blob_name)
        blob.upload_from_filename(path)
        return True
    except Exception as e:
        print(e)
        return False

#Download function for cloud

def download_cloud(blob_name, path, bucket_name):
    try:
        bucket =storage_client.get_bucket(bucket_name)
        blob = bucket.blob(blob_name)
        with open(path,'wb') as f:
            storage_client.download_blob_to_file(blob,f)
        return True
    except Exception as e:
        print(e)
        return False

#Getting image paths
ImgLocation="C:/Project/data3333/"

CATEGORIES = set(["malignant_all"])

ImagePaths=[]
for category in CATEGORIES:
    for image in list(os.listdir(ImgLocation+category)):
        ImagePaths=ImagePaths+[ImgLocation+category+"/"+image]
        
#Loading images and preprocessing

data_loaded=[]
for img in ImagePaths:
    image = cv2.imread(img)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_lowres = cv2.resize(image, (64, 64))
    data_loaded.append(image_lowres)
    
#Converting images to numpy array
data_loaded = np.array(data_loaded, dtype="float32") / 255.0

print("Shape of data_loaded: ", data_loaded.shape)

#Displaying some of the data

fig, axs = plt.subplots(2, 5, sharey=False, tight_layout=True, figsize=(16,9), facecolor='white')
n=0
for i in range(0,2):
    for j in range(0,5):
        axs[i,j].matshow(data_loaded[n])
        n=n+1
plt.show() 

#Setting up scaler between -1,1
scaler=MinMaxScaler(feature_range=(-1, 1))

data=data_loaded.copy()
print("Original shape of the data: ", data.shape)

#Reshaping data
data=data.reshape(-1, 1)
print("Reshaped data: ", data.shape)

#Fitting scaler
scaler.fit(data)

#Scaling data
data=scaler.transform(data)

#Reshaping to original shape
data=data.reshape(data_loaded.shape[0], 64, 64, 3)
print("Shape of the scaled array: ", data.shape)

#generator model

def generator(latent_dim):
    model = Sequential(name="Generator")
    
    
    n_nodes = 8 * 8 * 128 
    model.add(Dense(n_nodes, input_dim=latent_dim))
    model.add(Reshape((8, 8, 128)))

    model.add(Conv2DTranspose(filters=128, kernel_size=(4,4), strides=(2,2), padding='same'))
    model.add(LeakyReLU(alpha=0.3))
    Dropout(0.3),
                              
    model.add(Conv2DTranspose(filters=256, kernel_size=(4,4), strides=(2,2), padding='same'))
    model.add(LeakyReLU(alpha=0.3))
    Dropout(0.2),
    
    model.add(Conv2DTranspose(filters=512, kernel_size=(4,4), strides=(2,2), padding='same'))
    model.add(LeakyReLU(alpha=0.3))
    Dropout(0.1),
    
    model.add(Conv2D(filters=3, kernel_size=(5,5), activation='tanh', padding='same'))
    return model

latent_dim=100 
gen_model = generator(latent_dim)

#discriminator model

def discriminator(in_shape=(64,64,3)):
    model = Sequential(name="Discriminator") 
    
    model.add(Conv2D(filters=64, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape))
    model.add(LeakyReLU(alpha=0.2))
    
    model.add(Conv2D(filters=128, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape))
    model.add(LeakyReLU(alpha=0.2))
    
    model.add(Conv2D(filters=128, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape))
    model.add(LeakyReLU(alpha=0.2))
    
    model.add(Flatten())
    model.add(Dropout(0.3))
    model.add(Dense(1, activation='sigmoid'))
    
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001, beta_1=0.5), metrics=['accuracy'])
    return model

dis_model = discriminator()


#GAN model with both generator and discriminator

def def_gan(generator, discriminator):
    
    #Since we want to train the discriminator and generator differently, we deactivate the discriminator at first.
    discriminator.trainable = False
    
    #Adding generator and discriminator to the model
    model = Sequential(name="DCGAN")
    model.add(generator)
    model.add(discriminator)
    
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001, beta_1=0.5))
    return model

gan_model = def_gan(gen_model, dis_model)

#Getting real images and labelling

def real_samples(n, dataset):
    
    # Real images
    
    X = dataset[np.random.choice(dataset.shape[0], n, replace=True), :]

    #Labelling classes
    
    y = np.ones((n, 1))
    return X, y
    
    
def latent_vector(latent_dim, n):
    
    # Generating points in the latent dimension
    latent_input = np.random.randn(latent_dim * n)
    
    # Reshaping inputs
    latent_input = latent_input.reshape(n, latent_dim)
    return latent_input
  
#Fake sample generate function

def fake_samples(generator, latent_dim, n):
    
    # Generating points in the latent dimension
    latent_output = latent_vector(latent_dim, n)
    
    # Generating synthetic images
    X = generator.predict(latent_output)
    
    # Labeling synthetic samples
    y = np.zeros((n, 1))
    return X, y    

# Result analysis
def performance_summary(generator, discriminator, dataset, latent_dim, n=100):
    

    x_real, y_real = real_samples(n, dataset)

    _, real_accuracy = discriminator.evaluate(x_real, y_real, verbose=0)
    
    x_fake, y_fake = fake_samples(generator, latent_dim, n)

    _, fake_accuracy = discriminator.evaluate(x_fake, y_fake, verbose=0)
    
    print("Discriminator Accuracy on REAL images: ", real_accuracy)
    print("Discriminator Accuracy on FAKE (generated) images: ", fake_accuracy)
    
    # Displaying synthetic images
    x_fake_transform=x_fake.reshape(-1, 1)
    x_fake_transform=scaler.inverse_transform(x_fake_transform)
    x_fake_transform=x_fake_transform.reshape(n, 64, 64, 3)
    
    fig, axs = plt.subplots(2, 3, sharey=False, tight_layout=True, figsize=(12,6), facecolor='white')
    k=0
    for i in range(0,2):
        for j in range(0,3):
            axs[i,j].matshow(x_fake_transform[k])
            k=k+1
    plt.show() 

def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=10001, n_batch=32, n_eval=500):
    
    half_batch = int(n_batch / 2)
     
    for i in range(n_epochs):
    
    # Discriminator training
        x_real, y_real = real_samples(half_batch, dataset)
        x_fake, y_fake = fake_samples(g_model, latent_dim, half_batch)
        
        X, y = np.vstack((x_real, x_fake)), np.vstack((y_real, y_fake))
        discriminator_loss, _ = d_model.train_on_batch(X, y)
    
    # Generator training
        x_gan = latent_vector(latent_dim, n_batch)
        
        y_gan = np.ones((n_batch, 1))
        
        generator_loss = gan_model.train_on_batch(x_gan, y_gan)
        
        if (i) % n_eval == 0:
            print("Epoch number: ", i)
            print("Discriminator Loss ", discriminator_loss)
            print("Generator Loss: ", generator_loss)
            performance_summary(g_model, d_model, dataset, latent_dim)

#training
train(gen_model, dis_model, gan_model, data, latent_dim)

#generating 50000 synthetic data
number=50000
xfake, yfake = fake_samples(gen_model, 100, number)
xfake_transform=xfake.reshape(-1, 1)
xfake_transform=scaler.inverse_transform(xfake_transform)
xfake_transform=xfake_transform.reshape(number, 64, 64, 3)


k=0
#z=25000 #(Data generation is divided into 25000 and 25000 due to lack of memory.The z variable is used for the second part.)
for i in range(0,len(xfake_transform)):
    plt.imsave("C:\Project\saved3\malignant\%d.png"%k, xfake_transform[k])
    k=k+1
    #z=z+1

#Uploading sythetic images to cloud
path = r'C:\Project\saved3\malignant'
c=0
for c in range (0, 10):
    upload_cloud('%d'%c,os.path.join(path, '%d.png'%c), 'ganimages')
