import numpy as np 
import pandas as pd 
import os
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Dropout
from tensorflow.keras.utils import plot_model
from tensorflow.keras.optimizers import Adam 
import sys
import sklearn
from sklearn.preprocessing import MinMaxScaler
import matplotlib 
import matplotlib.pyplot as plt 
import tensorflow as tf
import cv2




ImgLocation="C:/Project/data3333/"


CATEGORIES = set(["malignant"])


ImagePaths=[]
for category in CATEGORIES:
    for image in list(os.listdir(ImgLocation+category)):
        ImagePaths=ImagePaths+[ImgLocation+category+"/"+image]
        

data_lowres=[]
for img in ImagePaths:
    image = cv2.imread(img)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_lowres = cv2.resize(image, (64, 64))
    data_lowres.append(image_lowres)
    

data_lowres = np.array(data_lowres, dtype="float") / 255.0

print("Shape of data_lowres: ", data_lowres.shape)



fig, axs = plt.subplots(2, 5, sharey=False, tight_layout=True, figsize=(16,9), facecolor='white')
n=0
for i in range(0,2):
    for j in range(0,5):
        axs[i,j].matshow(data_lowres[n])
        n=n+1
plt.show() 



scaler=MinMaxScaler(feature_range=(-1, 1))


data=data_lowres.copy()
print("Original shape of the data: ", data.shape)


data=data.reshape(-1, 1)
print("Reshaped data: ", data.shape)


scaler.fit(data)


data=scaler.transform(data)


data=data.reshape(data_lowres.shape[0], 64, 64, 3)
print("Shape of the scaled array: ", data.shape)



def generator(latent_dim):
    model = Sequential(name="Generator") # Model
    
    
    n_nodes = 8 * 8 * 128 # number of nodes in the first hidden layer
    model.add(Dense(n_nodes, input_dim=latent_dim, name='Generator-Hidden-Layer-1'))
    model.add(Reshape((8, 8, 128), name='Generator-Hidden-Layer-Reshape-1'))
    
   
    model.add(Conv2DTranspose(filters=128, kernel_size=(4,4), strides=(2,2), padding='same', name='Generator-Hidden-Layer-2'))
    model.add(ReLU(name='Generator-Hidden-Layer-Activation-2'))
                              
    
    model.add(Conv2DTranspose(filters=256, kernel_size=(4,4), strides=(2,2), padding='same', name='Generator-Hidden-Layer-3'))
    model.add(ReLU(name='Generator-Hidden-Layer-Activation-3'))
    
    
    model.add(Conv2DTranspose(filters=512, kernel_size=(4,4), strides=(2,2), padding='same', name='Generator-Hidden-Layer-4'))
    model.add(ReLU(name='Generator-Hidden-Layer-Activation-4'))
    
   
    model.add(Conv2D(filters=3, kernel_size=(5,5), activation='tanh', padding='same', name='Generator-Output-Layer'))
    return model


latent_dim=100 # Our latent space has 100 dimensions. We can change it to any number
gen_model = generator(latent_dim)



def discriminator(in_shape=(64,64,3)):
    model = Sequential(name="Discriminator") # Model
    

    model.add(Conv2D(filters=64, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape, name='Discriminator-Hidden-Layer-1'))
    model.add(LeakyReLU(alpha=0.2, name='Discriminator-Hidden-Layer-Activation-1'))
    
 
    model.add(Conv2D(filters=128, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape, name='Discriminator-Hidden-Layer-2'))
    model.add(LeakyReLU(alpha=0.2, name='Discriminator-Hidden-Layer-Activation-2'))
    
   
    model.add(Conv2D(filters=128, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape, name='Discriminator-Hidden-Layer-3'))
    model.add(LeakyReLU(alpha=0.2, name='Discriminator-Hidden-Layer-Activation-3'))
    
   
    model.add(Flatten(name='Discriminator-Flatten-Layer')) # Flatten the shape
    model.add(Dropout(0.3, name='Discriminator-Flatten-Layer-Dropout')) # Randomly drop some connections for better generalization
    model.add(Dense(1, activation='sigmoid', name='Discriminator-Output-Layer')) # Output Layer
    
    
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001, beta_1=0.5), metrics=['accuracy'])
    return model


dis_model = discriminator()



def def_gan(generator, discriminator):
    
    
    discriminator.trainable = False
    
   
    model = Sequential(name="DCGAN") # GAN Model
    model.add(generator) # Add Generator
    model.add(discriminator) # Add Disriminator
    
    
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001, beta_1=0.5))
    return model


gan_model = def_gan(gen_model, dis_model)




def real_samples(n, dataset):
    

    X = dataset[np.random.choice(dataset.shape[0], n, replace=True), :]


    y = np.ones((n, 1))
    return X, y
    
    
def latent_vector(latent_dim, n):
    

    latent_input = np.random.randn(latent_dim * n)
    

    latent_input = latent_input.reshape(n, latent_dim)
    return latent_input
  
    
def fake_samples(generator, latent_dim, n):
    

    latent_output = latent_vector(latent_dim, n)
    

    X = generator.predict(latent_output)
    
  
    y = np.zeros((n, 1))
    return X, y    



def performance_summary(generator, discriminator, dataset, latent_dim, n=50):
    

    x_real, y_real = real_samples(n, dataset)

    _, real_accuracy = discriminator.evaluate(x_real, y_real, verbose=0)
    

    x_fake, y_fake = fake_samples(generator, latent_dim, n)

    _, fake_accuracy = discriminator.evaluate(x_fake, y_fake, verbose=0)
    

    print("*** Evaluation ***")
    print("Discriminator Accuracy on REAL images: ", real_accuracy)
    print("Discriminator Accuracy on FAKE (generated) images: ", fake_accuracy)
    

    x_fake_inv_trans=x_fake.reshape(-1, 1)
    x_fake_inv_trans=scaler.inverse_transform(x_fake_inv_trans)
    x_fake_inv_trans=x_fake_inv_trans.reshape(n, 64, 64, 3)
    
    fig, axs = plt.subplots(2, 3, sharey=False, tight_layout=True, figsize=(12,6), facecolor='white')
    k=0
    for i in range(0,2):
        for j in range(0,3):
            axs[i,j].matshow(x_fake_inv_trans[k])
            k=k+1
    plt.show() 
    
    
    
def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=4001, n_batch=32, n_eval=100):
    
    
    half_batch = int(n_batch / 2)
    
   
    for i in range(n_epochs):
    
    
       
        x_real, y_real = real_samples(half_batch, dataset)
        
        x_fake, y_fake = fake_samples(g_model, latent_dim, half_batch)
        
        
        X, y = np.vstack((x_real, x_fake)), np.vstack((y_real, y_fake))
        discriminator_loss, _ = d_model.train_on_batch(X, y)
    
   
        
        x_gan = latent_vector(latent_dim, n_batch)
        
        
        y_gan = np.ones((n_batch, 1))
        
       
        generator_loss = gan_model.train_on_batch(x_gan, y_gan)
        
       
        if (i) % n_eval == 0:
            print("Epoch number: ", i)
            print("*** Training ***")
            print("Discriminator Loss ", discriminator_loss)
            print("Generator Loss: ", generator_loss)
            performance_summary(g_model, d_model, dataset, latent_dim)    



train(gen_model, dis_model, gan_model, data, latent_dim)
